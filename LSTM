import tensorflow as tf
print(tf.__version__)

# This is needed for the iterator over the data
# But not necessary if you have TF 2.0 installed
#!pip install tensorflow==2.0.0-beta0


#tf.enable_eager_execution()

# !pip install -q tensorflow-datasets
2.9.2
!pip install -q tensorflow-datasets
import tensorflow_datasets as tfds
imdb, info = tfds.load("imdb_reviews", with_info=True, as_supervised=True)
Downloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...
Dl Completed...: 0 url [00:00, ? url/s]
Dl Size...: 0 MiB [00:00, ? MiB/s]
Generating splits...:   0%|          | 0/3 [00:00, ? splits/s]
Generating train examples...:   0%|          | 0/25000 [00:00, ? examples/s]
Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteUWHB02/imdb_reviews-train.tfrecord*...…
Generating test examples...:   0%|          | 0/25000 [00:00, ? examples/s]
Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteUWHB02/imdb_reviews-test.tfrecord*...:…
Generating unsupervised examples...:   0%|          | 0/50000 [00:00, ? examples/s]
Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteUWHB02/imdb_reviews-unsupervised.tfrec…
Dataset imdb_reviews downloaded and prepared to ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.
import numpy as np

train_data, test_data = imdb['train'], imdb['test']

training_sentences = []
training_labels = []

testing_sentences = []
testing_labels = []

# str(s.tonumpy()) is needed in Python3 instead of just s.numpy()
for s,l in train_data:
  training_sentences.append(str(s.numpy()))
  training_labels.append(l.numpy())
  
for s,l in test_data:
  testing_sentences.append(str(s.numpy()))
  testing_labels.append(l.numpy())
  
training_labels_final = np.array(training_labels)
testing_labels_final = np.array(testing_labels)
vocab_size = 10000
embedding_dim = 16
max_length = 120
trunc_type='post'
oov_tok = ""


from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(training_sentences)
word_index = tokenizer.word_index
sequences = tokenizer.texts_to_sequences(training_sentences)
padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)

testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
testing_padded = pad_sequences(testing_sequences,maxlen=max_length)
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

def decode_review(text):
    return ' '.join([reverse_word_index.get(i, '?') for i in text])

print(decode_review(padded[1]))
print(training_sentences[1])
? ? ? ? ? ? ? b'i have been known to fall asleep during films but this is usually due to a combination of things including really tired being warm and comfortable on the  and having just eaten a lot however on this occasion i fell asleep because the film was rubbish the plot development was constant constantly slow and boring things seemed to happen but with no explanation of what was causing them or why i admit i may have missed part of the film but i watched the majority of it and everything just seemed to happen of its own  without any real concern for anything else i cant recommend this film at all '
b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),
    tf.keras.layers.Dense(6, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding (Embedding)       (None, 120, 16)           160000    
                                                                 
 bidirectional (Bidirectiona  (None, 64)               9600      
 l)                                                              
                                                                 
 dense (Dense)               (None, 6)                 390       
                                                                 
 dense_1 (Dense)             (None, 1)                 7         
                                                                 
=================================================================
Total params: 169,997
Trainable params: 169,997
Non-trainable params: 0
_________________________________________________________________
num_epochs = 50
history = model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))
Epoch 1/50
782/782 [==============================] - 24s 18ms/step - loss: 0.5039 - accuracy: 0.7318 - val_loss: 0.3663 - val_accuracy: 0.8432
Epoch 2/50
782/782 [==============================] - 13s 16ms/step - loss: 0.3073 - accuracy: 0.8764 - val_loss: 0.3566 - val_accuracy: 0.8464
Epoch 3/50
782/782 [==============================] - 13s 16ms/step - loss: 0.2444 - accuracy: 0.9061 - val_loss: 0.3624 - val_accuracy: 0.8420
Epoch 4/50
782/782 [==============================] - 13s 17ms/step - loss: 0.1980 - accuracy: 0.9268 - val_loss: 0.3961 - val_accuracy: 0.8337
Epoch 5/50
782/782 [==============================] - 13s 16ms/step - loss: 0.1544 - accuracy: 0.9455 - val_loss: 0.5060 - val_accuracy: 0.8307
Epoch 6/50
782/782 [==============================] - 14s 17ms/step - loss: 0.1118 - accuracy: 0.9622 - val_loss: 0.5290 - val_accuracy: 0.8226
Epoch 7/50
782/782 [==============================] - 13s 17ms/step - loss: 0.0737 - accuracy: 0.9757 - val_loss: 0.6345 - val_accuracy: 0.8155
Epoch 8/50
782/782 [==============================] - 13s 16ms/step - loss: 0.0468 - accuracy: 0.9854 - val_loss: 0.6795 - val_accuracy: 0.8110
Epoch 9/50
782/782 [==============================] - 13s 17ms/step - loss: 0.0346 - accuracy: 0.9893 - val_loss: 0.9656 - val_accuracy: 0.8083
Epoch 10/50
782/782 [==============================] - 13s 17ms/step - loss: 0.0304 - accuracy: 0.9901 - val_loss: 0.8139 - val_accuracy: 0.8096
Epoch 11/50
782/782 [==============================] - 13s 17ms/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.9798 - val_accuracy: 0.8116
Epoch 12/50
782/782 [==============================] - 13s 17ms/step - loss: 0.0162 - accuracy: 0.9951 - val_loss: 1.0520 - val_accuracy: 0.8104
Epoch 13/50
782/782 [==============================] - 14s 18ms/step - loss: 0.0176 - accuracy: 0.9946 - val_loss: 1.0000 - val_accuracy: 0.8054
Epoch 14/50
782/782 [==============================] - 13s 16ms/step - loss: 0.0104 - accuracy: 0.9968 - val_loss: 1.2009 - val_accuracy: 0.8042
Epoch 15/50
782/782 [==============================] - 13s 17ms/step - loss: 0.0127 - accuracy: 0.9954 - val_loss: 1.1694 - val_accuracy: 0.8071
Epoch 16/50
782/782 [==============================] - 13s 17ms/step - loss: 0.0131 - accuracy: 0.9952 - val_loss: 1.2119 - val_accuracy: 0.8099
Epoch 17/50
782/782 [==============================] - 13s 16ms/step - loss: 0.0068 - accuracy: 0.9976 - val_loss: 1.2704 - val_accuracy: 0.8024
Epoch 18/50
782/782 [==============================] - 13s 17ms/step - loss: 0.0098 - accuracy: 0.9967 - val_loss: 1.2646 - val_accuracy: 0.8044
Epoch 19/50
782/782 [==============================] - 13s 16ms/step - loss: 0.0097 - accuracy: 0.9968 - val_loss: 1.3558 - val_accuracy: 0.7986
Epoch 20/50
782/782 [==============================] - 13s 17ms/step - loss: 0.0062 - accuracy: 0.9979 - val_loss: 1.2475 - val_accuracy: 0.8048
Epoch 21/50
782/782 [==============================] - 13s 16ms/step - loss: 0.0060 - accuracy: 0.9978 - val_loss: 1.3761 - val_accuracy: 0.7930
Epoch 22/50
782/782 [==============================] - 13s 16ms/step - loss: 0.0046 - accuracy: 0.9984 - val_loss: 1.6094 - val_accuracy: 0.7967
Epoch 23/50
782/782 [==============================] - 13s 16ms/step - loss: 0.0091 - accuracy: 0.9966 - val_loss: 1.3186 - val_accuracy: 0.7978
Epoch 24/50
782/782 [==============================] - 13s 16ms/step - loss: 0.0058 - accuracy: 0.9982 - val_loss: 1.3783 - val_accuracy: 0.8039
Epoch 25/50
782/782 [==============================] - 13s 16ms/step - loss: 0.0054 - accuracy: 0.9982 - val_loss: 1.4422 - val_accuracy: 0.8024
Epoch 26/50
782/782 [==============================] - 13s 16ms/step - loss: 9.3534e-04 - accuracy: 0.9998 - val_loss: 1.4128 - val_accuracy: 0.7967
Epoch 27/50
782/782 [==============================] - 13s 16ms/step - loss: 0.0063 - accuracy: 0.9977 - val_loss: 1.5807 - val_accuracy: 0.8056
Epoch 28/50
782/782 [==============================] - 13s 17ms/step - loss: 0.0097 - accuracy: 0.9965 - val_loss: 1.4443 - val_accuracy: 0.7970
Epoch 29/50
782/782 [==============================] - 13s 16ms/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 1.6646 - val_accuracy: 0.7944
Epoch 30/50
782/782 [==============================] - 13s 17ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 1.4526 - val_accuracy: 0.8019
Epoch 31/50
782/782 [==============================] - 13s 16ms/step - loss: 2.7214e-04 - accuracy: 1.0000 - val_loss: 1.6094 - val_accuracy: 0.8069
Epoch 32/50
782/782 [==============================] - 13s 17ms/step - loss: 3.9952e-05 - accuracy: 1.0000 - val_loss: 1.6915 - val_accuracy: 0.8069
Epoch 33/50
782/782 [==============================] - 13s 17ms/step - loss: 2.3063e-05 - accuracy: 1.0000 - val_loss: 1.7446 - val_accuracy: 0.8069
Epoch 34/50
782/782 [==============================] - 13s 17ms/step - loss: 1.5932e-05 - accuracy: 1.0000 - val_loss: 1.8023 - val_accuracy: 0.8069
Epoch 35/50
782/782 [==============================] - 13s 16ms/step - loss: 1.1139e-05 - accuracy: 1.0000 - val_loss: 1.8558 - val_accuracy: 0.8071
Epoch 36/50
782/782 [==============================] - 13s 17ms/step - loss: 7.7104e-06 - accuracy: 1.0000 - val_loss: 1.9135 - val_accuracy: 0.8069
Epoch 37/50
782/782 [==============================] - 13s 17ms/step - loss: 5.2873e-06 - accuracy: 1.0000 - val_loss: 1.9711 - val_accuracy: 0.8069
Epoch 38/50
782/782 [==============================] - 13s 17ms/step - loss: 3.5923e-06 - accuracy: 1.0000 - val_loss: 2.0362 - val_accuracy: 0.8065
Epoch 39/50
782/782 [==============================] - 13s 17ms/step - loss: 2.4290e-06 - accuracy: 1.0000 - val_loss: 2.1003 - val_accuracy: 0.8064
Epoch 40/50
782/782 [==============================] - 13s 17ms/step - loss: 1.6132e-06 - accuracy: 1.0000 - val_loss: 2.1691 - val_accuracy: 0.8066
Epoch 41/50
782/782 [==============================] - 13s 16ms/step - loss: 1.0661e-06 - accuracy: 1.0000 - val_loss: 2.2380 - val_accuracy: 0.8060
Epoch 42/50
782/782 [==============================] - 14s 17ms/step - loss: 6.9805e-07 - accuracy: 1.0000 - val_loss: 2.3116 - val_accuracy: 0.8060
Epoch 43/50
782/782 [==============================] - 13s 17ms/step - loss: 4.5615e-07 - accuracy: 1.0000 - val_loss: 2.3833 - val_accuracy: 0.8057
Epoch 44/50
782/782 [==============================] - 13s 17ms/step - loss: 2.9514e-07 - accuracy: 1.0000 - val_loss: 2.4535 - val_accuracy: 0.8054
Epoch 45/50
782/782 [==============================] - 13s 16ms/step - loss: 1.9134e-07 - accuracy: 1.0000 - val_loss: 2.5272 - val_accuracy: 0.8052
Epoch 46/50
782/782 [==============================] - 13s 17ms/step - loss: 1.2336e-07 - accuracy: 1.0000 - val_loss: 2.5985 - val_accuracy: 0.8053
Epoch 47/50
782/782 [==============================] - 13s 17ms/step - loss: 8.0493e-08 - accuracy: 1.0000 - val_loss: 2.6686 - val_accuracy: 0.8050
Epoch 48/50
782/782 [==============================] - 13s 16ms/step - loss: 5.2232e-08 - accuracy: 1.0000 - val_loss: 2.7424 - val_accuracy: 0.8046
Epoch 49/50
782/782 [==============================] - 13s 17ms/step - loss: 3.4521e-08 - accuracy: 1.0000 - val_loss: 2.8099 - val_accuracy: 0.8044
Epoch 50/50
782/782 [==============================] - 13s 16ms/step - loss: 2.3110e-08 - accuracy: 1.0000 - val_loss: 2.8797 - val_accuracy: 0.8042
import matplotlib.pyplot as plt


def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()

plot_graphs(history, 'accuracy')
plot_graphs(history, 'loss')


# Model Definition with LSTM
model_lstm = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(6, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model_lstm.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model_lstm.summary()
Model: "sequential_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_1 (Embedding)     (None, 120, 16)           160000    
                                                                 
 bidirectional_1 (Bidirectio  (None, 64)               12544     
 nal)                                                            
                                                                 
 dense_2 (Dense)             (None, 6)                 390       
                                                                 
 dense_3 (Dense)             (None, 1)                 7         
                                                                 
=================================================================
Total params: 172,941
Trainable params: 172,941
Non-trainable params: 0
_________________________________________________________________
num_epochs = 10
history_lstm = model_lstm.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))
Epoch 1/10
782/782 [==============================] - 13s 17ms/step - loss: 0.1114 - accuracy: 0.9626 - val_loss: 0.5629 - val_accuracy: 0.8233
Epoch 2/10
782/782 [==============================] - 13s 17ms/step - loss: 0.0899 - accuracy: 0.9713 - val_loss: 0.6895 - val_accuracy: 0.8073
Epoch 3/10
782/782 [==============================] - 13s 17ms/step - loss: 0.0666 - accuracy: 0.9803 - val_loss: 0.7459 - val_accuracy: 0.8189
Epoch 4/10
782/782 [==============================] - 14s 18ms/step - loss: 0.0597 - accuracy: 0.9821 - val_loss: 0.8321 - val_accuracy: 0.8175
Epoch 5/10
782/782 [==============================] - 13s 17ms/step - loss: 0.0471 - accuracy: 0.9857 - val_loss: 0.9880 - val_accuracy: 0.8201
Epoch 6/10
782/782 [==============================] - 13s 17ms/step - loss: 0.0409 - accuracy: 0.9871 - val_loss: 0.9313 - val_accuracy: 0.8144
Epoch 7/10
782/782 [==============================] - 13s 17ms/step - loss: 0.0346 - accuracy: 0.9895 - val_loss: 1.0359 - val_accuracy: 0.8139
Epoch 8/10
782/782 [==============================] - 13s 17ms/step - loss: 0.0252 - accuracy: 0.9933 - val_loss: 1.1014 - val_accuracy: 0.8113
Epoch 9/10
782/782 [==============================] - 13s 17ms/step - loss: 0.0221 - accuracy: 0.9936 - val_loss: 1.1856 - val_accuracy: 0.8109
Epoch 10/10
782/782 [==============================] - 13s 17ms/step - loss: 0.0196 - accuracy: 0.9940 - val_loss: 1.0427 - val_accuracy: 0.8093
# Model Definition with Conv1D
model_cnn = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.Conv1D(128, 5, activation='relu'),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(6, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model_cnn.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model_cnn.summary()
Model: "sequential_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding_3 (Embedding)     (None, 120, 16)           160000    
                                                                 
 conv1d_1 (Conv1D)           (None, 116, 128)          10368     
                                                                 
 global_average_pooling1d_1   (None, 128)              0         
 (GlobalAveragePooling1D)                                        
                                                                 
 dense_6 (Dense)             (None, 6)                 774       
                                                                 
 dense_7 (Dense)             (None, 1)                 7         
                                                                 
=================================================================
Total params: 171,149
Trainable params: 171,149
Non-trainable params: 0
_________________________________________________________________
num_epochs = 10
history_cnn = model_cnn.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))
Epoch 1/10
782/782 [==============================] - 10s 8ms/step - loss: 0.4535 - accuracy: 0.7724 - val_loss: 0.3457 - val_accuracy: 0.8497
Epoch 2/10
782/782 [==============================] - 5s 6ms/step - loss: 0.2796 - accuracy: 0.8862 - val_loss: 0.3766 - val_accuracy: 0.8340
Epoch 3/10
782/782 [==============================] - 6s 7ms/step - loss: 0.2263 - accuracy: 0.9116 - val_loss: 0.3814 - val_accuracy: 0.8398
Epoch 4/10
782/782 [==============================] - 5s 6ms/step - loss: 0.1909 - accuracy: 0.9274 - val_loss: 0.4077 - val_accuracy: 0.8358
Epoch 5/10
782/782 [==============================] - 5s 6ms/step - loss: 0.1609 - accuracy: 0.9428 - val_loss: 0.4717 - val_accuracy: 0.8216
Epoch 6/10
782/782 [==============================] - 6s 7ms/step - loss: 0.1317 - accuracy: 0.9546 - val_loss: 0.4973 - val_accuracy: 0.8237
Epoch 7/10
782/782 [==============================] - 5s 6ms/step - loss: 0.1080 - accuracy: 0.9660 - val_loss: 0.5785 - val_accuracy: 0.8160
Epoch 8/10
782/782 [==============================] - 6s 7ms/step - loss: 0.0863 - accuracy: 0.9749 - val_loss: 0.6700 - val_accuracy: 0.8105
Epoch 9/10
782/782 [==============================] - 5s 6ms/step - loss: 0.0726 - accuracy: 0.9796 - val_loss: 0.7385 - val_accuracy: 0.8096
Epoch 10/10
782/782 [==============================] - 6s 7ms/step - loss: 0.0575 - accuracy: 0.9836 - val_loss: 0.8233 - val_accuracy: 0.8085
